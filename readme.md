
# ðŸ”Š VoiceGuard AI: Deepfake Audio Detector

![License](https://img.shields.io/badge/license-MIT-blue)
![Python](https://img.shields.io/badge/python-3.8%2B-blue)
![Streamlit](https://img.shields.io/badge/framework-Streamlit-orange)

> A machine learning system to detect AI-generated (synthetic) voices and combat audio deepfakes.  
> Uses MFCCs, spectral features, and XGBoost with SHAP explainability.


## ðŸŽ¯ Purpose
Voice cloning and deepfakes are rising threats in misinformation, fraud, and identity theft.  
**VoiceGuard AI** helps detect synthetic voices using machine learning â€” while emphasizing **transparency** and **ethical use**.

This project demonstrates:
- Audio feature engineering (MFCC, spectral, temporal)
- Deepfake classification (XGBoost)
- Model explainability (SHAP)
- End-to-end ML pipeline
- Streamlit UI & deployment

---

## ðŸš€ Features

- ðŸŽ¤ Record voice via browser microphone
- ðŸ“¤ Upload `.wav` or `.mp3` files
- ðŸ” Detects if audio is **real (human)** or **fake (AI-generated)**
- ðŸ§  SHAP-based explanation: see *why* the model made its decision
- ðŸ“Š Visualizations: waveform, spectrogram, confusion matrix
- ðŸ§ª Educational demo: compare real vs. cloned voice samples
- âš ï¸ Ethical disclaimer and responsible use policy

---

## ðŸ§° Tech Stack

| Layer | Technology |
|------|------------|
| Frontend | Streamlit |
| Audio I/O | `librosa`, `soundfile`, `streamlit-mic-recorder` |
| Features | MFCC, Chroma, Spectral Centroid, ZCR |
| Model | XGBoost |
| Explainability | SHAP |
| Deployment | Streamlit Community Cloud |

---

## ðŸ“¦ Installation & Setup

```bash
# Clone the repo
git clone https://github.com/yourname/voice-deepfake-detector.git
cd voice-deepfake-detector

# Create virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate   # Windows

# Install dependencies
pip install -r requirements.txt

# Train the detection model (creates model/deepfake_detector.pkl)
python model/train_detector.py
```

---

## â–¶ï¸ Run Locally

```bash
streamlit run app.py
```

Open http://localhost:8501 in your browser.

---

## â˜ï¸ Deploy to Streamlit Cloud

1. Push your code to a **public GitHub repo**
2. Go to [https://streamlit.io/cloud](https://streamlit.io/cloud)
3. Sign in and **add your repo**
4. Set main file: `app.py`
5. Click "Deploy"

âœ… Your app will be live in 2â€“3 minutes!

> Note: `model/train_detector.py` runs on startup to generate the model if not present.

---

## ðŸ›‘ Ethical Use Notice

This tool is for **education, research, and security awareness only**.  
Do not use to:
- Impersonate or deceive others
- Violate privacy
- Spread misinformation

Voice cloning is powerful â€” use it responsibly.

---

## ðŸŒŸ Future Improvements

- Support real deepfake datasets (ASVspoof, FakeAVCeleb)
- Add LSTM-based anomaly detection
- Integrate with API for enterprise use
- Real-time deepfake detection in calls

---

## ðŸ™Œ Acknowledgements

- [Librosa](https://librosa.org/) for audio processing
- [SHAP](https://shap.readthedocs.io/) for explainability
- [Streamlit](https://streamlit.io/) for rapid UI development

---

## ðŸ“„ License

MIT Â© [Your Name]
```

> ðŸ’¡ Save this as `README.md`  
> ðŸ’¡ Create an `assets/` folder and add screenshots (you can use Streamlitâ€™s UI to capture them)

---

## ðŸš€ Step 2: Deploy to Streamlit Community Cloud

### âœ… Steps:
1. Create a **GitHub repo** and push your project
2. Go to [https://streamlit.io/cloud](https://streamlit.io/cloud)
3. Click **"New App"**
4. Select your repo
5. Branch: `main`
6. Main file path: `app.py`
7. Click **Deploy**

> âš ï¸ It will run `pip install -r requirements.txt` and start `app.py`  
> âœ… First run: `train_detector.py` auto-generates model

---

## ðŸŒŸ Step 3: Killer Enhancement â€” Real-Time Feedback (Optional but Impressive)

Add **live audio analysis** from mic â€” not just after recording.

Update `app.py` with:
```python
# In Record tab
if st.button("ðŸŽ¤ Analyze Live (Simulated)"):
    with st.spinner("Simulating real-time analysis..."):
        # Show live confidence growing
        progress_bar = st.progress(0)
        status_text = st.empty()
        result_text = st.empty()

        for i in range(1, 11):
            import time
            time.sleep(0.3)
            progress_bar.progress(i * 10)
            status_text.info(f"Analyzing {i}/10 frames...")
        
        # Final result
        result_text.success("**Final Verdict: Real (Human)** | Confidence: 94%")
```

Or use WebRTC for true real-time (advanced).


